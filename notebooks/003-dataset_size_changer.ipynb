{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b11019",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange, tqdm\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adults\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# from src.evaluation import data_benchmark\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# from src.models import vae_keras\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# from src.datasets import adults#, dataset_utils\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(tf.config.list_physical_devices('GPU'))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# tf.debugging.set_log_device_placement(True)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/tab-ddpm/src/datasets/adults.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# from tensorflow.keras import layers, Model, Input, losses\u001b[39;00m\n\u001b[1;32m     13\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfnlwgt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation-num\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarital-status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapital-gain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapital-loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhours-per-week\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Model, Input, losses, layers, optimizers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from matplotlib.colors import BoundaryNorm\n",
    "# import seaborn as sns\n",
    "\n",
    "# import itertools\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets import adults\n",
    "# from src.evaluation import data_benchmark\n",
    "# from src.models import vae_keras\n",
    "# from src.datasets import adults#, dataset_utils\n",
    "\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# import tensorflow as tf\n",
    "# for gpu in tf.config.list_physical_devices('GPU'):\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e093f8b",
   "metadata": {},
   "source": [
    "### This is just a scrappy notebook to temporarily reduce the dataset size of files in the ~\\data\\adult folder\n",
    "#### (Restoring the original sizes requires copypasting from a backup of the original files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6bb16d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(array([0, 1], dtype=int64), array([19775,  6273], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "y_t = np.load('../data/adult/y_train.npy', allow_pickle=True)\n",
    "print(type(y_t))\n",
    "print(np.unique(y_t, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "168bcd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[30. 42.  4. ... 40. 40. 40.]\n",
      "12546\n",
      "(12546,)\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n",
      " 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 70. 72. 73. 75.\n",
      " 76. 77. 78. 80. 81. 82. 84. 85. 87. 88. 90. 96. 97. 98. 99.]\n",
      "[  19214.   19302.   19520. ... 1226583. 1268339. 1455435.]\n",
      "<class 'numpy.ndarray'>\n",
      "[30. 42.  4. ... 40. 43. 40.]\n",
      "26048\n",
      "(26048,)\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n",
      " 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 70. 72. 73. 75.\n",
      " 76. 77. 78. 80. 81. 82. 84. 85. 86. 87. 88. 89. 90. 91. 92. 94. 95. 96.\n",
      " 97. 98. 99.]\n",
      "[  18827.   19214.   19302. ... 1268339. 1455435. 1484705.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_cat = np.load('../data/adult/X_cat_train.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# print(len(data))\n",
    "# print(len(data[0]))\n",
    "# print(data)\n",
    "print(type(data))\n",
    "print(data[:,-1])\n",
    "print(len(data[:,-1]))\n",
    "print((data[:,-1].shape))\n",
    "print(np.sort(np.unique(data[:,-1])))\n",
    "print(np.sort(np.unique(data[:,1])))\n",
    "# y = data[:,-1]\n",
    "# for i in range(len(y)):\n",
    "#     print(y[i])\n",
    "#     print(type(y[i]))\n",
    "# print(type(np.unique(data[:,-1])[0]))\n",
    "\n",
    "import numpy as np\n",
    "data = np.load('../data/adult/X_num_train.npy', allow_pickle=True)\n",
    "\n",
    "# print(len(data))\n",
    "# print(len(data[0]))\n",
    "# print(data)\n",
    "print(type(data))\n",
    "print(data[:,-1])\n",
    "print(len(data[:,-1]))\n",
    "print((data[:,-1].shape))\n",
    "print(np.sort(np.unique(data[:,-1])))\n",
    "print(np.sort(np.unique(data[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e5b0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a Test dataset that contains an equal amounts of each class\n",
    "# y should contain only two classes 0 and 1\n",
    "def TrainSplitEqualBinary(X, y, samples_n): #samples_n per class\n",
    "    \n",
    "    indicesClass1 = []\n",
    "    indicesClass2 = []\n",
    "    \n",
    "    for i in range(0, len(y)):\n",
    "        if y[i] == 0 and len(indicesClass1) < samples_n:\n",
    "            indicesClass1.append(i)\n",
    "        elif y[i] == 1 and len(indicesClass2) < samples_n:\n",
    "            indicesClass2.append(i)\n",
    "            \n",
    "        if len(indicesClass1) == samples_n and len(indicesClass2) == samples_n:\n",
    "            break\n",
    "    \n",
    "    X_test_class1 = X[indicesClass1]\n",
    "    X_test_class2 = X[indicesClass2]\n",
    "    \n",
    "    X_test = np.concatenate((X_test_class1,X_test_class2), axis=0)\n",
    "    \n",
    "    #remove x_test from X\n",
    "    X_train = np.delete(X, indicesClass1 + indicesClass2, axis=0)\n",
    "    \n",
    "    Y_test_class1 = y[indicesClass1]\n",
    "    Y_test_class2 = y[indicesClass2]\n",
    "    \n",
    "    y_test = np.concatenate((Y_test_class1,Y_test_class2), axis=0)\n",
    "    \n",
    "    #remove y_test from y\n",
    "    y_train = np.delete(y, indicesClass1 + indicesClass2, axis=0)\n",
    "    \n",
    "    if (X_test.shape[0] != 2 * samples_n or y_test.shape[0] != 2 * samples_n):\n",
    "        raise Exception(\"Problem with split 1!\")\n",
    "        \n",
    "    if (X_train.shape[0] + X_test.shape[0] != X.shape[0] or y_train.shape[0] + y_test.shape[0] != y.shape[0]):\n",
    "        raise Exception(\"Problem with split 2!\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b002fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26048, 8) (26048, 6) (26048,)\n",
      "(12546, 14)\n",
      "['United-States' 'El-Salvador' 'United-States' ... 'United-States'\n",
      " 'United-States' 'United-States']\n"
     ]
    }
   ],
   "source": [
    "for split in ['train']: #, 'test', 'val']:\n",
    "    x_cat = np.load('../data/adult/X_cat_' + split +'.npy', allow_pickle=True) # 8 columns\n",
    "    x_num = np.load('../data/adult/X_num_' + split +'.npy', allow_pickle=True) # 6 columns\n",
    "    y = np.load('../data/adult/y_' + split+ '.npy', allow_pickle=True)\n",
    "    \n",
    "    print(x_cat.shape, x_num.shape, y.shape)\n",
    "    \n",
    "    x_train = np.concatenate([x_cat, x_num], axis=1)\n",
    "    \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x_train, y, train_size=1024, stratify=y)\n",
    "    # 6273\n",
    "    _, x_train, _, y_train = TrainSplitEqualBinary(x_train, y, 6273) \n",
    "\n",
    "    \n",
    "    \n",
    "    np.save('../data/adult/X_cat_' + split +'.npy', x_train[:,:8]) \n",
    "    np.save('../data/adult/X_num_' + split +'.npy', x_train[:,8:].astype(np.float32)) \n",
    "    np.save('../data/adult/y_' + split+ '.npy', y_train) \n",
    "\n",
    "    (print(x_train.shape))\n",
    "\n",
    "    print(x_cat[:,-1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a290d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['United-States' 'El-Salvador' 'United-States' ... 'United-States'\n",
      " 'United-States' 'United-States']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(x_cat[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74c92a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 14)\n",
      "(1024,)\n",
      "(16281, 14)\n",
      "(16281,)\n",
      "(6513, 14)\n",
      "(6513,)\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'test', 'val']:\n",
    "    x_cat = np.load('X_cat_' + split +'.npy', allow_pickle=True) # 8 columns\n",
    "    x_num = np.load('X_num_' + split +'.npy', allow_pickle=True) # 6 columns\n",
    "    y = np.load('y_' + split+ '.npy', allow_pickle=True)\n",
    "    \n",
    "    x_train = np.concatenate([x_cat, x_num], axis=1)\n",
    "    (print(x_train.shape))\n",
    "    (print(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d662b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
