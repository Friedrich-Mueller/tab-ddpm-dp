{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8fdf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5443af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Model, Input, losses, layers, optimizers\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "# from src.datasets import adults\n",
    "# from src.models import vae_keras\n",
    "# from src.datasets import adults#, dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec7499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_borders(series, nbin, drop_duplicates=True):\n",
    "    \"\"\"Returns a list of bin borders\n",
    "    e.g. generates a list of 10 values for 9 bins (for [0,1])\n",
    "    \"\"\"\n",
    "    \n",
    "    bin_borders = [0]\n",
    "    \n",
    "    vals = np.sort(series.to_numpy())\n",
    "    if drop_duplicates==True:\n",
    "        vals = list(dict.fromkeys(vals)) # drop duplicates\n",
    "\n",
    "    bin_depth = len(vals)/nbin\n",
    "    for i in range(1, nbin):\n",
    "        border_value = vals[int(i*bin_depth)]\n",
    "        bin_borders.append(border_value)\n",
    "    bin_borders.append(1.0)\n",
    "    \n",
    "    return bin_borders\n",
    "\n",
    "def value_to_bin_index(x, bin_borders):\n",
    "    \"\"\"Returns the index of the bin a value belongs to\n",
    "    e.g. a list with 10 binning borders consists of 9 bins/indices\n",
    "    \"\"\"\n",
    "    for i in range(len(bin_borders)-1):\n",
    "        if (x>=bin_borders[i] and x<bin_borders[i+1]):\n",
    "            return i+1\n",
    "    if x == 1.0:\n",
    "        return 19\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    if x > 1.0:\n",
    "        return 20\n",
    "    raise Exception(\"value cannot belong to any bin\", x, i, bin_borders, len(bin_borders))\n",
    "    \n",
    "def cramers_v_corrected(ftable):\n",
    "    \"\"\"Computes Cramer's V with bias correction\n",
    "    to measure the correlation between two attributes.\n",
    "    Input is a frequency table between 2 attributes.\n",
    "    Returns Cramers V value â‚¬ [0, 1]\n",
    "    \"\"\"\n",
    "    n = np.sum(ftable)\n",
    "    k, r = ftable.shape\n",
    "    X2 = sp.stats.chi2_contingency(ftable)[0] # Pearson's chi-squared test statistic\n",
    "    phi2 = X2 / n\n",
    "    \n",
    "    phitilde2 = max(0, phi2 - (k - 1) * (r - 1) / (n - 1))\n",
    "    ktilde = k - (k - 1)**2 / (n - 1)\n",
    "    rtilde = r - (r - 1)**2 / (n - 1)\n",
    "\n",
    "    V = np.sqrt(phitilde2 / min(ktilde - 1, rtilde - 1))\n",
    "       \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a37618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TVD_ind(data_orig, data_synth, cont_cols, cat_cols, nbin=19, drop_duplicates=True):\n",
    "    \"\"\" \n",
    "    Generates equal-depth bins based on the original data.\n",
    "    Calculates the TVD between two one-dimensional distributions.\n",
    "    Returns the 1 minus the average TVD over all marginals.\n",
    "    \"\"\"\n",
    "    \n",
    "    tvds = []\n",
    "    \n",
    "    # calculate TVDs for cont columns\n",
    "    for col in cont_cols:\n",
    "        \n",
    "        # discretize/bin based on original data\n",
    "        bin_borders = binning_borders(data_orig[col], nbin, drop_duplicates)\n",
    "\n",
    "        vals_orig = data_orig[col].to_numpy()\n",
    "        vals_synth = data_synth[col].to_numpy()\n",
    "        bin_counts_orig = []\n",
    "        bin_counts_synth = []\n",
    "        for i in range(nbin):\n",
    "            \n",
    "            col_bin_orig = [x for x in vals_orig if (x >= bin_borders[i] and x < bin_borders[i+1])]\n",
    "            if i == (nbin-1):\n",
    "                col_bin_orig = [x for x in vals_orig if (x >= bin_borders[i] and x <= bin_borders[i+1])]\n",
    "            \n",
    "            col_bin_synth = [x for x in vals_synth if (x >= bin_borders[i] and x < bin_borders[i+1])]\n",
    "            if i == (nbin-1):\n",
    "                col_bin_synth = [x for x in vals_synth if (x >= bin_borders[i] and x <= bin_borders[i+1])]            \n",
    "            \n",
    "            bin_counts_orig.append(len(col_bin_orig))\n",
    "            bin_counts_synth.append(len(col_bin_synth))\n",
    "            \n",
    "        # add additional bins to each end of the range for synth data outside of range\n",
    "        bin_counts_orig = [0] + bin_counts_orig + [0]\n",
    "        bin_counts_synth = [len([x for x in vals_synth if x < bin_borders[0]])] + bin_counts_synth + [len([x for x in vals_synth if x > bin_borders[nbin]])]\n",
    "            \n",
    "        # calculate TVD for columns\n",
    "        a = np.array(bin_counts_orig/np.sum(bin_counts_orig)).reshape(nbin+2,1)\n",
    "        b = np.array(bin_counts_synth/np.sum(bin_counts_synth)).reshape(nbin+2,1)\n",
    "        tvds.append(0.5 * sum(abs(a - b))[0])\n",
    "        \n",
    "    # calculate TVDs for cat columns\n",
    "    for col in cat_cols:\n",
    "\n",
    "        counts_orig_lst = []\n",
    "        counts_synth_lst = []\n",
    "        \n",
    "        counts_orig = data_orig[col].value_counts(normalize=True)\n",
    "        counts_synth = data_synth[col].value_counts(normalize=True)\n",
    "        \n",
    "        # align counts and treat non-occuring cat values in synth data\n",
    "        for index, value in counts_orig.items():\n",
    "            counts_orig_lst.append(value)\n",
    "            if index in counts_synth:\n",
    "                counts_synth_lst.append(counts_synth[index])\n",
    "            else:\n",
    "                counts_synth_lst.append(0)\n",
    "                \n",
    "        # calculate TVD for column\n",
    "        a = np.array(counts_orig_lst).reshape(-1,1)\n",
    "        b = np.array(counts_synth_lst).reshape(-1,1)\n",
    "        tvds.append(0.5 * sum(abs(a - b))[0]) \n",
    "                \n",
    "    print('\\n1-TVD (Ind)')\n",
    "    print(1-np.average(tvds))\n",
    "    return 1-np.average(tvds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcfb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TVD_pair(data_orig, data_synth, cont_cols, cat_cols, nbin=19, drop_duplicates=True):\n",
    "    \"\"\"\n",
    "    Generates equal-depth bins based on the original data.\n",
    "    Calculates the TVD for each two-way marginal.\n",
    "    Returns the 1 minus the average TVD over all marginals.\n",
    "    \"\"\"\n",
    "    \n",
    "    tvds = []\n",
    "        \n",
    "    data_o = data_orig.copy()\n",
    "    data_s = data_synth.copy()\n",
    "    \n",
    "    # discretize cont cols\n",
    "    for col in cont_cols:\n",
    "               \n",
    "        # bin based on original data\n",
    "        bin_borders = binning_borders(data_o[col], nbin, drop_duplicates)\n",
    "        data_o[col] = data_o[col].apply(value_to_bin_index, bin_borders=bin_borders)\n",
    "        data_s[col] = data_s[col].apply(value_to_bin_index, bin_borders=bin_borders)\n",
    "        \n",
    "        # bin counts\n",
    "        counts_orig = data_o[col].value_counts(normalize=False)\n",
    "        counts_synth = data_s[col].value_counts(normalize=False)\n",
    "        \n",
    "    cols = cont_cols + cat_cols\n",
    "    \n",
    "    for i in range(len(cols)-1):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            \n",
    "            marginals1 = pd.crosstab(data_o[cols[i]], data_o[cols[j]], margins=False, normalize=True)\n",
    "            marginals2 = pd.crosstab(data_s[cols[i]], data_s[cols[j]], margins=False, normalize=True)\n",
    "            \n",
    "            marginals_subtracted = marginals1.subtract(marginals2)\n",
    "            \n",
    "            # drop NaNs which occur when subtraction includes value counts of 0 (e.g.  Holand-Netherlands 1 )\n",
    "            marginals_subtracted = marginals_subtracted.dropna(axis=1, how='all')\n",
    "            marginals_subtracted = marginals_subtracted.dropna(axis=0, how='all')\n",
    "\n",
    "            \n",
    "            tvds.append(0.5*marginals_subtracted.abs().to_numpy().sum())\n",
    "    \n",
    "    print('\\n1-TVD (Pair)')\n",
    "    print(1-np.average(tvds))\n",
    "    return 1-np.average(tvds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245e6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_v_levels(data, cat_cols, cont_cols=None):\n",
    "    \"\"\"\n",
    "    Generates a list of correlation values for each pair of columns of a given dataset.\n",
    "    Sticks to the convention of 4 degrees of correlation, depending on the magnitude of V.\n",
    "    [0, .1) is low\n",
    "    [.1, .3) is weak\n",
    "    [.3, .5) is middle\n",
    "    [.5, 1) is strong\n",
    "    \"\"\"\n",
    "    \n",
    "    V_levels = []\n",
    "    \n",
    "#     data = df.copy()\n",
    "    \n",
    "#     # discretize cont cols\n",
    "#     for col in cont_cols:\n",
    "               \n",
    "#         # bin based on original data\n",
    "#         bin_borders = binning_borders(data[col], nbin, drop_duplicates)\n",
    "#         data[col] = data[col].apply(value_to_bin_index, bin_borders=bin_borders)\n",
    "        \n",
    "# #         counts_orig = data_o[col].value_counts(normalize=True)\n",
    "#         counts = data[col].value_counts(normalize=False)\n",
    "\n",
    "        \n",
    "        \n",
    "#         print(' ')\n",
    "#         print(col)\n",
    "#         print(bin_borders) \n",
    "#         print(counts)\n",
    "        \n",
    "    cols = cat_cols\n",
    "    \n",
    "    for i in range(len(cols)):\n",
    "        row = []\n",
    "        for j in range(len(cols)):\n",
    "            \n",
    "            frequency_table = pd.crosstab(data[cols[i]], data[cols[j]])\n",
    "            \n",
    "            V = cramers_v_corrected(frequency_table.to_numpy())\n",
    "            \n",
    "            # assign levels to values\n",
    "            if V > 0.5:\n",
    "                row.append(1)\n",
    "            elif V > 0.3:\n",
    "                row.append(0.5)\n",
    "            elif V > 0.1:\n",
    "                row.append(0.3)\n",
    "            else:\n",
    "                row.append(0.1)\n",
    "        V_levels.append(row)\n",
    "\n",
    "    return np.array(V_levels)\n",
    "\n",
    "def cramer_v_coracc(Vs_orig, Vs_synth):\n",
    "    \"\"\"Returns CorAcc metric reporting fraction of pairs where\n",
    "    synth and orig data assign same correlation level.\n",
    "    \"\"\"\n",
    "    h = len(Vs_orig)\n",
    "    n = Vs_orig.size\n",
    "    equals = ((Vs_orig == Vs_synth).sum() - h ) / 2\n",
    "    total = ((h**2)-h)/2\n",
    "    \n",
    "    return equals/total\n",
    "\n",
    "def cramer_corr_heatmap_cat(data_o, data_s, cat_cols):\n",
    "    \n",
    "    cramer_Vs_o = cramer_v_levels(data_o, cat_cols)\n",
    "    cramer_Vs_s = cramer_v_levels(data_s, cat_cols)\n",
    "    \n",
    "    df_o = pd.DataFrame(cramer_Vs_o, index=cat_cols, columns=cat_cols)\n",
    "    df_s = pd.DataFrame(cramer_Vs_s, index=cat_cols, columns=cat_cols)\n",
    "\n",
    "    corr_acc = \"{:.2f}\".format(cramer_v_coracc(cramer_Vs_o, cramer_Vs_s))\n",
    "    norm = BoundaryNorm([0, 0.1000001, 0.3000001, 0.5000001, 1.0],5)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, gridspec_kw=dict(width_ratios=[2,2.5]), figsize=(13, 5))\n",
    "    \n",
    "    ax1.set_title(\"Cramer V Correlation between Variables\\n Original Data CorAcc = \" + str(1));\n",
    "    sns.heatmap(df_o, cmap=sns.cubehelix_palette(5, as_cmap=False), linewidths=0.2, cbar=False, ax=ax1)\n",
    "    \n",
    "    ax2.set_title(\"Cramer V Correlation between Variables\\n Synthetic Data CorAcc = \" + str(corr_acc)); \n",
    "    sns.heatmap(df_s, cmap=sns.cubehelix_palette(5, as_cmap=False), linewidths=0.2, cbar=True, ax=ax2,\n",
    "                norm=norm, cbar_kws=dict(spacing=\"proportional\") )   \n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901897f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_classifier_F1(data_orig, data_synth, grid_search=None, enc_dict=None):\n",
    "    \"\"\"\n",
    "    Generic XGBoost classifier.\n",
    "    Expects normalized continuous and numeric categorical data in numpy array format.\n",
    "    Alternatively, also expects normalized continuous and numeric categorical data in\n",
    "    numpy array format and encoding dict to perform one hot encoding.\n",
    "    \n",
    "    Runs the classification task:\n",
    "    - once trained on and classified with the original data\n",
    "    - once trained on synthetic data and classified with original data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### train on and classify original data\n",
    "    print(\"\\nOriginal data F1 macro score:\")\n",
    "    X_train = data_orig.iloc[:, 0:-1]\n",
    "    y_train = data_orig.iloc[:, -1]\n",
    "    \n",
    "    X_test = X_train\n",
    "    y_test = y_train\n",
    "    \n",
    "    # run with numeric categorical features\n",
    "        \n",
    "    # fit model to training data\n",
    "    xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    f1 = f1_score(y_test, predictions, average='macro') *100\n",
    "    print(\"F1: %.2f%%\" % (f1))\n",
    "\n",
    "    # run with one-hot-encoded categorical features     \n",
    "    if enc_dict is not None:\n",
    "        \n",
    "        # one hot encoding\n",
    "        X_train, col_idxs = adults.one_hot_encoding(data_orig.iloc[:, 0:-1], enc_dict)\n",
    "        y_train = data_orig.iloc[:, -1]\n",
    "\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "\n",
    "\n",
    "        # fit model to training data\n",
    "        xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "        # make predictions for test data\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy_ = accuracy_score(y_test, predictions)\n",
    "        print(\"\\nAccuracy OHE'ed: %.2f%%\" % (accuracy_ * 100.0))\n",
    "        f1_ = f1_score(y_test, predictions, average='macro') *100\n",
    "        print(\"F1 OHE'ed: %.2f%%\" % (f1_))\n",
    "        \n",
    "        \n",
    "    #### train on synthetic data and classify original data\n",
    "    print(\"\\nSynthetic data F1 macro score:\")\n",
    "    X_train = data_synth.iloc[:, 0:-1]\n",
    "    y_train = data_synth.iloc[:, -1]\n",
    "    \n",
    "    X_test = data_orig.iloc[:, 0:-1]\n",
    "    y_test = data_orig.iloc[:, -1]\n",
    "    \n",
    "    # run with numeric categorical features\n",
    "    \n",
    "    # fit model to training data\n",
    "    xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    f1 = f1_score(y_test, predictions, average='macro') *100\n",
    "    print(\"F1: %.2f%%\" % (f1))\n",
    "\n",
    "    # run with one-hot-encoded categorical features     \n",
    "    if enc_dict is not None:\n",
    "        print(\"\\nOne hot encoded:\")\n",
    "        \n",
    "        # one hot encoding\n",
    "        X_train, col_idxs = adults.one_hot_encoding(data_orig.iloc[:, 0:-1], enc_dict)\n",
    "        y_train = data_orig.iloc[:, -1]\n",
    "\n",
    "        X_test, col_idxs = adults.one_hot_encoding(data_synth.iloc[:, 0:-1], enc_dict)\n",
    "        y_test = data_synth.iloc[:, -1]\n",
    "\n",
    "\n",
    "        # fit model to training data\n",
    "        xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "        # make predictions for test data\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy_ = accuracy_score(y_test, predictions)\n",
    "        print(\"Accuracy OHE'ed: %.2f%%\" % (accuracy_ * 100.0))\n",
    "        f1_ = f1_score(y_test, predictions, average='macro') *100\n",
    "        print(\"F1 OHE'ed: %.2f%%\" % (f1_))\n",
    "\n",
    "\n",
    "    if grid_search is not None:\n",
    "        \"\"\"\n",
    "        Does the same as above, but peforms grid search to optimize\n",
    "        number of estimators.\n",
    "        \"\"\"\n",
    "\n",
    "        ### train on and classify original data\n",
    "        print(\"\\nOriginal data F1 macro score:\")\n",
    "        X_train = data_orig.iloc[:, 0:-1]\n",
    "        y_train = data_orig.iloc[:, -1]\n",
    "\n",
    "        X_test = X_train\n",
    "        y_test = y_train\n",
    "\n",
    "        # run with numeric categorical features\n",
    "        for i in [50,100,200,400,800,1600]:\n",
    "\n",
    "            # fit model no training data\n",
    "            xgb_model = XGBClassifier(n_estimators=i,\n",
    "                                      learning_rate=0.05,\n",
    "                                      early_stopping_rounds=i-i*.75).fit(X_train,\n",
    "                                                                      y_train,\n",
    "                                                                      eval_set=[(X_test,y_test)],\n",
    "                                                                      verbose=False)\n",
    "\n",
    "            # make predictions for test data\n",
    "            y_pred = xgb_model.predict(X_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "\n",
    "            # evaluate predictions\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            print(\"Accuracy: %.2f%% --- %.2f N_estimators\" % (accuracy * 100.0, i))\n",
    "            f1 = f1_score(y_test, predictions, average='macro') *100\n",
    "            print(\"F1: %.2f%%\" % (f1))\n",
    "\n",
    "        # run with one-hot-encoded categorical features     \n",
    "        if enc_dict is not None:\n",
    "            print(\"\\nOne hot encoded:\")\n",
    "            for i in [50,100,200,400,800,1600]:\n",
    "\n",
    "                # fit model no training data\n",
    "                xgb_model = XGBClassifier(n_estimators=i,\n",
    "                                          learning_rate=0.05,\n",
    "                                          early_stopping_rounds=i-i*.75).fit(X_train,\n",
    "                                                                          y_train,\n",
    "                                                                          eval_set=[(X_test,y_test)],\n",
    "                                                                          verbose=False)\n",
    "\n",
    "                # one hot encoding\n",
    "                X_train, col_idxs = adults.one_hot_encoding(data_orig.iloc[:, 0:-1], enc_dict)\n",
    "                y_train = data_orig.iloc[:, -1]\n",
    "\n",
    "                X_test = X_train\n",
    "                y_test = y_train\n",
    "\n",
    "\n",
    "                # fit model to training data\n",
    "                xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "                # make predictions for test data\n",
    "                y_pred = xgb_model.predict(X_test)\n",
    "                predictions = [round(value) for value in y_pred]\n",
    "\n",
    "                # evaluate predictions\n",
    "                accuracy_ = accuracy_score(y_test, predictions)\n",
    "                print(\"Accuracy OHE'ed: %.2f%% --- %.2f N_estimators\" % (accuracy_ * 100.0, i))\n",
    "                f1_ = f1_score(y_test, predictions, average='macro') *100\n",
    "                print(\"F1 OHE'ed: %.2f%%\" % (f1_))\n",
    "\n",
    "\n",
    "        ### train on synthetic data and classify original data\n",
    "        print(\"\\nSynthetic data F1 macro score:\")\n",
    "        X_train = data_synth.iloc[:, 0:-1]\n",
    "        y_train = data_synth.iloc[:, -1]\n",
    "\n",
    "        X_test = data_orig.iloc[:, 0:-1]\n",
    "        y_test = data_orig.iloc[:, -1]\n",
    "\n",
    "        # run with numeric categorical features\n",
    "        for i in [50,100,200,400,800,1600]:\n",
    "\n",
    "            # fit model no training data\n",
    "            xgb_model = XGBClassifier(n_estimators=i,\n",
    "                                      learning_rate=0.05,\n",
    "                                      early_stopping_rounds=i-i*.75).fit(X_train,\n",
    "                                                                      y_train,\n",
    "                                                                      eval_set=[(X_test,y_test)],\n",
    "                                                                      verbose=False)\n",
    "            # make predictions for test data\n",
    "            y_pred = xgb_model.predict(X_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "\n",
    "            # evaluate predictions\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            print(\"Accuracy: %.2f%% --- %.2f N_estimators\" % (accuracy * 100.0, i))\n",
    "            f1 = f1_score(y_test, predictions, average='macro') *100\n",
    "            print(\"F1: %.2f%%\" % (f1))\n",
    "\n",
    "        # run with one-hot-encoded categorical features     \n",
    "        if enc_dict is not None:\n",
    "            print(\"\\nOne hot encoded:\")\n",
    "            for i in [50,100,200,400,800,1600]:\n",
    "\n",
    "                # fit model no training data\n",
    "                xgb_model = XGBClassifier(n_estimators=i,\n",
    "                                          learning_rate=0.05,\n",
    "                                          early_stopping_rounds=i-i*.75).fit(X_train,\n",
    "                                                                          y_train,\n",
    "                                                                          eval_set=[(X_test,y_test)],\n",
    "                                                                          verbose=False)\n",
    "\n",
    "                # one hot encoding\n",
    "                X_train, col_idxs = adults.one_hot_encoding(data_orig.iloc[:, 0:-1], enc_dict)\n",
    "                y_train = data_orig.iloc[:, -1]\n",
    "\n",
    "                X_test, col_idxs = adults.one_hot_encoding(data_synth.iloc[:, 0:-1], enc_dict)\n",
    "                y_test = data_synth.iloc[:, -1]\n",
    "\n",
    "\n",
    "                # fit model to training data\n",
    "                xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "                # make predictions for test data\n",
    "                y_pred = xgb_model.predict(X_test)\n",
    "                predictions = [round(value) for value in y_pred]\n",
    "\n",
    "                # evaluate predictions\n",
    "                accuracy_ = accuracy_score(y_test, predictions)\n",
    "                print(\"Accuracy OHE'ed: %.2f%% --- %.2f N_estimators\" % (accuracy_ * 100.0, i))\n",
    "                f1_ = f1_score(y_test, predictions, average='macro') *100\n",
    "                print(\"F1 OHE'ed: %.2f%%\" % (f1_))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370719ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         XGBoost_classifier_F1(data_orig, data_synth, grid_search, enc_dict)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Load adult data for test purposes\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m all_data, enc_dict, dec_dict, Scaler, cont_cols, cat_cols \u001b[38;5;241m=\u001b[39m \u001b[43madults\u001b[49m\u001b[38;5;241m.\u001b[39mload_adults_data()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# split data in two for test purposes\u001b[39;00m\n\u001b[1;32m     35\u001b[0m data_synth, data_orig \u001b[38;5;241m=\u001b[39m train_test_split(all_data, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)    \n",
      "\u001b[0;31mNameError\u001b[0m: name 'adults' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_synthetic_data(data_orig, data_synth, cont_cols, cat_cols, metrics, grid_search=None, enc_dict=None):\n",
    "    \"\"\"\n",
    "    Takes an original dataset and a synthetic dataset generated from\n",
    "    an algorithm that is trained on the former.\n",
    "    Continuous values must be normalized.\n",
    "    Categorical values must be mapped to numerical features.\n",
    "    \n",
    "    Returns 3 statistical metrics including:\n",
    "    - average 1-TVD over all one-way marginals\n",
    "    - average 1-TVD oder all two-way marginals\n",
    "    - correlation accuracy (fraction of pairs of same correlation level)\n",
    "    \n",
    "    Returns the classification accuracy (F1 score using macro average) \n",
    "    of an XGBoost classifier trained on the synthetic data, used to\n",
    "    make predictions on the original data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Statistical metrics\n",
    "    if metrics[0]:\n",
    "        TVD_ind(data_synth, data_orig, cont_cols, cat_cols)\n",
    "    if metrics[1]:\n",
    "        TVD_pair(data_synth, data_orig, cont_cols, cat_cols)\n",
    "    if metrics[2]:\n",
    "        cramer_corr_heatmap_cat(data_orig, data_synth, cat_cols)\n",
    "    \n",
    "    # Classification accuracy\n",
    "    if metrics[3]:\n",
    "        XGBoost_classifier_F1(data_orig, data_synth, grid_search, enc_dict)\n",
    "    \n",
    "    \n",
    "# Load adult data for test purposes\n",
    "all_data, enc_dict, dec_dict, Scaler, cont_cols, cat_cols = adults.load_adults_data()\n",
    "\n",
    "# split data in two for test purposes\n",
    "data_synth, data_orig = train_test_split(all_data, train_size=0.5)    \n",
    "    \n",
    "# run the 3 statistical metrics:\n",
    "print(\"\\nrun the 3 statistical metrics:\")\n",
    "evaluate_synthetic_data(data_orig, data_synth, cont_cols, cat_cols, [1,1,1,0])\n",
    "\n",
    "# run classification accuracy metric (without one hot encoding):\n",
    "print(\"\\nrun classification accuracy metric:\")\n",
    "evaluate_synthetic_data(data_orig, data_synth, cont_cols, cat_cols, [0,0,0,1])\n",
    "\n",
    "# # run classification accuracy metric (with and without one hot encoding):\n",
    "# print(\"\\nrun classification accuracy metric (with and without one hot encoding):\")\n",
    "# evaluate_synthetic_data(data_orig, data_synth, cont_cols, cat_cols, [0,0,0,1], enc_dict=enc_dict)\n",
    "\n",
    "# # run classification accuracy metric with grid search (with and without one hot encoding):\n",
    "# print(\"\\nrun classification accuracy metric with grid search (with and without one hot encoding):\")\n",
    "# evaluate_synthetic_data(data_orig, data_synth, cont_cols, cat_cols, [0,0,0,1], grid_search=True, enc_dict=enc_dict)\n",
    "\n",
    "# run classification accuracy metric with grid search (without one hot encoding):\n",
    "print(\"\\nrun classification accuracy metric with grid search:\")\n",
    "evaluate_synthetic_data(data_orig, data_synth, cont_cols, cat_cols, [0,0,0,1], grid_search=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce6b29-7d5b-4997-a2b2-6fd366fb7b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
